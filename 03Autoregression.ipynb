{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a344836-2055-4318-b7ca-62f093c5cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 Autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7f433e-8aff-4fcb-8a4b-697e5b57f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "weatherclimateED = pd.read_csv('weatherclimateED.csv', parse_dates = [0], dayfirst = True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366d1977-f012-4005-9a58-dd6b010c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epiweeks Module converts dates to CDC Epiweek format\n",
    "## Further documentation on https://pypi.org/project/epiweeks/\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0f4c3f-fbee-4b7f-8197-c3948ab6c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 866 entries, 200601 to 202231\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                                              Non-Null Count  Dtype  \n",
      "---  ------                                                              --------------  -----  \n",
      " 0   Cardiovascular disease                                              679 non-null    float64\n",
      " 1   Chronic respiratory disease                                         679 non-null    float64\n",
      " 2   Diabetes mellitus                                                   679 non-null    float64\n",
      " 3   Digestive disease                                                   679 non-null    float64\n",
      " 4   Endocrine disorders                                                 679 non-null    float64\n",
      " 5   Factors influencing health status and contact with health services  679 non-null    float64\n",
      " 6   Genitourinary disorders                                             679 non-null    float64\n",
      " 7   Ill-defined diseases                                                679 non-null    float64\n",
      " 8   Infectious and Parasitic Diseases                                   679 non-null    float64\n",
      " 9   Malignant neoplasms                                                 679 non-null    float64\n",
      " 10  Musculoskeletal disease                                             679 non-null    float64\n",
      " 11  Neurological and sense disorders                                    679 non-null    float64\n",
      " 12  Oral Diseases                                                       679 non-null    float64\n",
      " 13  Other neoplasms                                                     679 non-null    float64\n",
      " 14  Respiratory Infection                                               679 non-null    float64\n",
      " 15  Skin diseases                                                       679 non-null    float64\n",
      " 16  MaxT                                                                574 non-null    float64\n",
      " 17  MeanT                                                               574 non-null    float64\n",
      " 18  MinT                                                                574 non-null    float64\n",
      " 19  RH                                                                  574 non-null    float64\n",
      " 20  AH                                                                  574 non-null    float64\n",
      " 21  pm25                                                                574 non-null    float64\n",
      " 22  pm10                                                                574 non-null    float64\n",
      " 23  o3                                                                  574 non-null    float64\n",
      " 24  no2                                                                 574 non-null    float64\n",
      " 25  so2                                                                 574 non-null    float64\n",
      " 26  co                                                                  574 non-null    float64\n",
      " 27  Rain                                                                574 non-null    float64\n",
      "dtypes: float64(28)\n",
      "memory usage: 196.2+ KB\n"
     ]
    }
   ],
   "source": [
    "## This section creates a full complete dataset that includes all the variables of interest that will be used\n",
    "## iloc function selects the relevant variables of interest based on column number\n",
    "## Problematic weather columns (i.e. don't select!): 6, 16, 17, 19, 20\n",
    "## Disease columns excluded due to limited dataset: 21:24, 25\n",
    "\n",
    "weatherclimateED['epiweek'] = weatherclimateED['Date'].apply(create_epiweek)\n",
    "weatherclimateED = weatherclimateED.set_index('epiweek')\n",
    "weatherclimateED = weatherclimateED.iloc[:, np.r_[30:32, 33:39, 40, 42 , 45:47, 49:51,  52:54, 1:6, 8:15]]\n",
    "weatherclimateED.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3f3296-097d-40b0-a7c2-6d598a519f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This function takes the full dataset and creates an initial dataset with the specified range\n",
    "## also returns the name of the target variable for creation of the initial dataset\n",
    "## note disease_var here is an integer based off the column number\n",
    "def create_initial_dataset(dataset, disease_var: int):\n",
    "    explore_df = dataset.copy()\n",
    "    range_start = Week(2009,1)\n",
    "    range_end = Week (2018,52)\n",
    "    explore_df = explore_df.loc[range_start:range_end]\n",
    "    target_var = explore_df.columns.values.tolist()[disease_var]\n",
    "\n",
    "    if not os.path.exists(target_var):\n",
    "        os.makedirs(target_var)\n",
    "    path = os.path.join(target_var, F'initial_dataset.csv')\n",
    "    \n",
    "    explore_df.to_csv(path)\n",
    "    #explore_df.info()\n",
    "    #explore_df\n",
    "\n",
    "    return explore_df, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc107e1c-6d9b-4139-a100-b1726b6162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive(dataset, target_var):\n",
    "    naive = dataset.copy()\n",
    "    naive = naive[[target_var]].shift(1)\n",
    "    return naive.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3b1deb-e97f-45d7-b5f7-6c4f296caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3672d89c-abc7-4fa5-8e1b-5c6bfc8fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step is the number of weeks ahead that we are forecasting, e.g. step=2 is 2 weeks ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var):\n",
    "    stepped_dataset = dataset.copy()\n",
    "    y = stepped_dataset[[target_var]].shift(-step+1)\n",
    "    if step != 1:\n",
    "        X = stepped_dataset.iloc[:-step+1, :]\n",
    "    else:\n",
    "        X = stepped_dataset\n",
    "    return X.drop(target_var, axis = 1), y.dropna()\n",
    "## So now target variable (y variable for exploration) is shifted back by 2 weeks. i.e., taking the y-value from 2 weeks later\n",
    "## and setting it to the current index. So linear regression of y+2 with the current X values. X will have\n",
    "## a smaller dataset with the last 2 time points removed because of the shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7112b902-bf3c-426a-ac21-d98a3bd3a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439f8bdf-0d86-42b3-a937-6094d334ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "def errors(model, model_name, X_train, y_train, errors_path, filename):\n",
    "    if model_name == 'naive':\n",
    "        y_train_errors = pd.DataFrame(X_train - y_train)\n",
    "    else:\n",
    "        y_train_errors = pd.DataFrame(model.predict(X_train) - y_train)\n",
    "    #print(y_train_errors)\n",
    "    #y_train_errors will contain list of errors in training set from window_start to window_end\n",
    "    #right before window_end+1 in the filename\n",
    "    errors_path = os.path.join(errors_path, model_name)\n",
    "    if not os.path.exists(errors_path):\n",
    "        os.makedirs(errors_path)\n",
    "    y_train_errors_path = os.path.join(errors_path, F'{filename}.csv')\n",
    "    y_train_errors.to_csv(y_train_errors_path)\n",
    "\n",
    "def coefs(model, coefs_path, filename):\n",
    "    coefs_path\n",
    "\n",
    "## This function runs the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def regression_with_naive(X_dataset, y_dataset, window_start, window_end, y_pred, y_params, errors_path, test_length, naive, target_var, y_ridge, y_lasso):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models\n",
    "        y_train = y.loc[window_start:window_end].values.ravel()\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "        ## evaluate variance\n",
    "        '''\n",
    "    \n",
    "        ## Naive Forecast\n",
    "        # add the [0] to extract as float, and not as series\n",
    "        y_pred.at[window_end+1, 'naive'] = naive.loc[window_end+1][0]\n",
    "\n",
    "        errors(naive, 'naive', naive.loc[window_start:window_end].values.ravel(), y_train, errors_path, window_end+1)\n",
    "        \n",
    "        \n",
    "        ## Linear Regression Model\n",
    "        linreg_model = LinearRegression()\n",
    "        # Fit the model to the training data\n",
    "        linreg_model.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'linreg'] = linreg_model.predict(X_test)\n",
    "\n",
    "        errors(linreg_model, 'linreg', X_train, y_train, errors_path, window_end+1)\n",
    "        '''\n",
    "    \n",
    "        ## Implement cross-validation split\n",
    "        tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "        ## Ridge model\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        \n",
    "        #y_pred.at[window_end+1, 'ridge'] = ridge_model.predict(X_test)\n",
    "        #y_params.at[window_end+1, 'ridge_alpha'] = ridge_cv.alpha_\n",
    "\n",
    "        alpha = ridge_cv.alpha_\n",
    "        ridge_edf = 0\n",
    "        for d in ridge_model.coef_:\n",
    "            ridge_edf += d**2/(d**2+alpha)\n",
    "        \n",
    "        y_ridge.at[window_end+1, 'ridge_edf'] = ridge_edf\n",
    "        \n",
    "        #errors(ridge_model, 'ridge', X_train, y_train, errors_path, window_end+1)\n",
    "\n",
    "        \n",
    "        ## Lasso Model\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        #y_pred.at[window_end+1, 'lasso'] = lasso_model.predict(X_test)\n",
    "        #y_params.at[window_end+1, 'lasso_alpha'] = lasso_cv.alpha_\n",
    "        #y_params.at[window_end+1, 'lasso_n_iter'] = lasso_cv.n_iter_\n",
    "        \n",
    "        y_lasso.at[window_end+1, 'lasso_edf'] = np.count_nonzero(lasso_model.coef_)\n",
    "\n",
    "        #errors(lasso_model, 'lasso', X_train, y_train, errors_path, window_end+1)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        ## ElasticNet Model\n",
    "        elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "        elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "        # Create the ElasticNet model with the optimal l1 and alpha values\n",
    "        elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "        elasticnet_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'elasticnet'] = elasticnet_model.predict(X_test)\n",
    "        \n",
    "        y_params.at[window_end+1, 'elasticnet_alpha'] = elasticnet_cv.alpha_\n",
    "        y_params.at[window_end+1, 'elasticnet_l1ratio'] = elasticnet_cv.l1_ratio_\n",
    "\n",
    "        errors(elasticnet_model, 'elasticnet', X_train, y_train, errors_path, window_end+1)\n",
    "        \n",
    "        ## Random Forest\n",
    "        randomforest_model = RandomForestRegressor(n_estimators = 1000, max_features = 'sqrt', random_state = 18)\n",
    "        randomforest_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'randomforest'] = randomforest_model.predict(X_test)\n",
    "        errors(randomforest_model, 'randomforest', X_train, y_train, errors_path, window_end+1)\n",
    "    \n",
    "        ## Gradient Boost\n",
    "        gradientboost_model = GradientBoostingRegressor(n_estimators = 1000, random_state = 18)\n",
    "        gradientboost_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'gradientboost'] = gradientboost_model.predict(X_test)\n",
    "        errors(gradientboost_model, 'gradientboost', X_train, y_train, errors_path, window_end+1)\n",
    "    \n",
    "        ## KNN\n",
    "        knn_parameters = {'n_neighbors': range(1, 40), 'weights': ['uniform', 'distance']}\n",
    "        #round(len(y_train)*0.15-1))\n",
    "        knn_cv = GridSearchCV(KNeighborsRegressor(), knn_parameters, cv = tscv)\n",
    "        knn_cv.fit(X_train, y_train)\n",
    "        #knn_model = knn_cv.predict(X_test)\n",
    "        #knn_model = KNeighborsRegressor()\n",
    "        #knn_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'knn'] = knn_cv.predict(X_test)\n",
    "        y_params.at[window_end+1, 'knn_n'] = knn_cv.best_estimator_\n",
    "        errors(knn_cv, 'knn', X_train, y_train, errors_path, window_end+1)\n",
    "        '''\n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "        if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "            print(F'{target_var} done with {window_end+1}; {count} out of {test_length}')\n",
    "            \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "    print(F'The last epiweek for {target_var} to be predicted is: {window_end}')\n",
    "    print(F'The total number of predicted epiweeks for {target_var} is: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12edf56-31b5-4b42-a957-7df729f4892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function sets up the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def run_first_order_regression(dataset, lag, step, target_var, window_perc):\n",
    "    print(F'Running first order regression for {target_var} lag {lag} step {step}')\n",
    "    \n",
    "    naive = create_naive(dataset, target_var)\n",
    "    #print(naive.info())\n",
    "    \n",
    "    lagged_dataset = create_lagged_dataset(dataset, lag, target_var)\n",
    "    #print(lagged_dataset)\n",
    "    \n",
    "    X, y = create_stepped_dataset(lagged_dataset, step, target_var)\n",
    "    #print(X.info())\n",
    "    #print(y)\n",
    "    \n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "\n",
    "    print(F'The first epiweek to be predicted for {target_var} lag {lag} step {step} is: {window_end+1}')\n",
    "    \n",
    "    y_pred = create_output_dataset(y, window_end)\n",
    "    y_params = create_output_dataset(y, window_end)\n",
    "    y_ridge = create_output_dataset(y, window_end)\n",
    "    y_lasso = create_output_dataset(y, window_end)\n",
    "\n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "    print(F'The initial training dataset length for {target_var} lag {lag} step {step} is: {train_length}')\n",
    "\n",
    "\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    print(F'The initial testing dataset length for {target_var} lag {lag} step {step} is: {test_length}')\n",
    "\n",
    "    errors_path = os.path.join(target_var, 'errors', F'L{lag}_S{step}')\n",
    "    \n",
    "    if not os.path.exists(errors_path):\n",
    "        os.makedirs(errors_path)\n",
    "        \n",
    "    regression_with_naive(X, y, window_start, window_end, y_pred, y_params, errors_path, test_length, naive, target_var, y_ridge, y_lasso)\n",
    "\n",
    "    #pred_path = os.path.join(target_var, 'pred')\n",
    "    #params_path = os.path.join(target_var, 'params')\n",
    "    \n",
    "    ridge_path = os.path.join(target_var, 'ridge_param')\n",
    "    lasso_path = os.path.join(target_var, 'lasso_param')\n",
    "\n",
    "    '''\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "    if not os.path.exists(params_path):\n",
    "        os.makedirs(params_path)\n",
    "    '''\n",
    "    if not os.path.exists(ridge_path):\n",
    "        os.makedirs(ridge_path)\n",
    "    if not os.path.exists(lasso_path):\n",
    "        os.makedirs(lasso_path)\n",
    "\n",
    "    #pred_path = os.path.join(pred_path, F'L{lag}_S{step}.csv')\n",
    "    #params_path = os.path.join(params_path, F'L{lag}_S{step}.csv')\n",
    "    ridge_path = os.path.join(ridge_path, F'L{lag}_S{step}.csv')\n",
    "    lasso_path = os.path.join(lasso_path, F'L{lag}_S{step}.csv')    \n",
    "\n",
    "\n",
    "    \n",
    "    #y_pred.to_csv(pred_path)\n",
    "    #y_params.to_csv(params_path)\n",
    "\n",
    "    y_ridge.to_csv(ridge_path)\n",
    "    y_lasso.to_csv(lasso_path)\n",
    "    \n",
    "\n",
    "    print(F'Completed for {target_var} lag {lag} step {step}')\n",
    "    clear_output(wait=False)\n",
    "    return y_ridge, y_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f1d5bb5-c6e2-44dd-bab9-fb440ba32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function runs the regression for one disease, for all lags and steps, hence the for loop\n",
    "\n",
    "def run_disease_regression(dataset, disease_var, lag_start, lag_end, step_start, step_end):\n",
    "    \n",
    "    ## Note how the integer disease_var is input into this function, and then\n",
    "    ## the string target_var is returned for the remaining functions\n",
    "    explore_df, target_var = create_initial_dataset(dataset, disease_var)\n",
    "\n",
    "    with open(\"target_variables.txt\") as target_variables_file:\n",
    "        if target_var not in target_variables_file.read():\n",
    "            with open(\"target_variables.txt\", 'a') as target_variables_file:\n",
    "                target_variables_file.write(F'{target_var}\\n')\n",
    "    \n",
    "    ## run the first order regression for all lags and steps for this target variable\n",
    "    print(F'Running regression for {target_var}')\n",
    "    for lag in range(lag_start, lag_end):\n",
    "        for step in range(step_start, step_end):\n",
    "            run_first_order_regression(explore_df, lag = lag, step = step, target_var = target_var, window_perc = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91c8eb6-6b76-4038-8d84-52d9e70b81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deprecated as now using the Parallel and run_disease_regression functions\n",
    "'''\n",
    "def run_full_regression(dataset, disease_list, lag_start, lag_end, step_start, step_end):\n",
    "    for disease_var in disease_list:\n",
    "        run_disease_regression(dataset, disease_var, lag_start, lag_end, step_start, step_end)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6787b52d-4052-4a42-9be8-b5e233ea441e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   1 tasks      | elapsed: 38.0min\n",
      "[Parallel(n_jobs=-2)]: Done   2 out of  16 | elapsed: 41.2min remaining: 288.2min\n",
      "[Parallel(n_jobs=-2)]: Done   3 out of  16 | elapsed: 214.1min remaining: 927.9min\n",
      "[Parallel(n_jobs=-2)]: Done   4 out of  16 | elapsed: 218.0min remaining: 654.0min\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of  16 | elapsed: 232.7min remaining: 511.8min\n",
      "[Parallel(n_jobs=-2)]: Done   6 out of  16 | elapsed: 253.2min remaining: 421.9min\n",
      "[Parallel(n_jobs=-2)]: Done   7 out of  16 | elapsed: 260.9min remaining: 335.4min\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  16 | elapsed: 557.8min remaining: 557.8min\n",
      "[Parallel(n_jobs=-2)]: Done   9 out of  16 | elapsed: 576.1min remaining: 448.1min\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  16 | elapsed: 582.8min remaining: 349.7min\n",
      "[Parallel(n_jobs=-2)]: Done  11 out of  16 | elapsed: 610.8min remaining: 277.6min\n",
      "[Parallel(n_jobs=-2)]: Done  12 out of  16 | elapsed: 766.0min remaining: 255.3min\n",
      "[Parallel(n_jobs=-2)]: Done  13 out of  16 | elapsed: 801.1min remaining: 184.9min\n",
      "[Parallel(n_jobs=-2)]: Done  14 out of  16 | elapsed: 815.6min remaining: 116.5min\n",
      "[Parallel(n_jobs=-2)]: Done  16 out of  16 | elapsed: 946.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done  16 out of  16 | elapsed: 946.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Main function call using Parallel\n",
    "## x in range (0,16) represents the 16 diseases that are the target variables. However, for this function we input them as integers\n",
    "## the create_initial_dataset function will convert the integer format to string format\n",
    "## Using parallel, each disease can be run on one computer core\n",
    "Parallel(n_jobs=-2, verbose=51)(delayed(run_disease_regression)(weatherclimateED, x, 8, 9, 1, 13) for x in range(0,16))\n",
    "#run_full_regression(weatherclimateED, range(0,16), 8, 9, 1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9f7b9-b0ba-40a7-ae8c-f6d8398e8c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
