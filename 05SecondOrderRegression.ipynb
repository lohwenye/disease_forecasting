{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688e0739-63a1-41a0-b141-0c14b4fd16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#05 Second Order Regression\n",
    "## This code takes the prediction forecast output from 03Autoregression and runs a second order regression\n",
    "## In the first order (03Autoregression) case: disease and weather variables were the X values for predicting Y - target variable\n",
    "## In the second order case (this one): prediction forecasts from first order, i.e., Naive Forecast, Linear Regression, LASSO, Ridge, etc\n",
    "## are the new X values for predicting Y the target variable. \n",
    "\n",
    "## During this second order regression, we run the same forecast methods, e.g. Linear, LASSO, Ridge, etc, a second time\n",
    "## In the same way that in first order regression the forecast methods help us choose which disease/weather variables best predict the target variable,\n",
    "## In second order regression, the forecast methods help us choose which forecast methods best predict the target variable. \n",
    "\n",
    "## The code here is almost identical to 03Autoregression, but it takes in the inputs from 03Autoregression prediction outputs.\n",
    "## Additionally, naive forecast method is omitted from second order. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36da9eb-5739-4339-a3b8-33921b69b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358166ea-9279-469c-b7ad-61ab5fe36aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e77b682-8966-4400-a03c-6bd96ebe6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_y(dataset, target_var):\n",
    "    X_and_y = dataset.copy()\n",
    "    return X_and_y.drop(target_var, axis = 1), X_and_y[[target_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b5c670-b9f2-4db7-a472-9ed315465772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c59ce0-3930-4ddf-9ee6-fc1040d2bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "def regression_without_naive(X_dataset, y_dataset, window_start, window_end, y_pred, y_params, test_length):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models\n",
    "        y_train = y.loc[window_start:window_end].values.ravel()\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "        ## evaluate variance\n",
    "    \n",
    "        ## Naive Forecast N/A for second order regression\n",
    "        # add the [0] to extract as float, and not as series\n",
    "        #y_pred.at[window_end+1, 'naive'] = naive.loc[window_end+1][0]\n",
    "        \n",
    "        ## Linear Regression Model\n",
    "        linreg_model = LinearRegression()\n",
    "        # Fit the model to the training data\n",
    "        linreg_model.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'linreg_2'] = linreg_model.predict(X_test)\n",
    "\n",
    "        \n",
    "    \n",
    "        ## Implement cross-validation split\n",
    "        tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "        ## Ridge model\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'ridge_2'] = ridge_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'ridge_2_alpha'] = ridge_cv.alpha_\n",
    "    \n",
    "        ## Lasso Model\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'lasso_2'] = lasso_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'lasso_2_alpha'] = lasso_cv.alpha_\n",
    "    \n",
    "        ## ElasticNet Model\n",
    "        elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "        elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "        # Create the ElasticNet model with the optimal l1 and alpha values\n",
    "        elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "        elasticnet_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'elasticnet_2'] = elasticnet_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'elasticnet_2_alpha'] = elasticnet_cv.alpha_\n",
    "        y_params.at[window_end+1, 'elasticnet_2_l1ratio'] = elasticnet_cv.l1_ratio_\n",
    "        \n",
    "        ## Random Forest\n",
    "        randomforest_model = RandomForestRegressor(n_estimators = 1000, max_features = 'sqrt', random_state = 18)\n",
    "        randomforest_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'randomforest_2'] = randomforest_model.predict(X_test)\n",
    "    \n",
    "        ## Gradient Boost\n",
    "        gradientboost_model = GradientBoostingRegressor(n_estimators = 1000, random_state = 18)\n",
    "        gradientboost_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'gradientboost_2'] = gradientboost_model.predict(X_test)\n",
    "    \n",
    "        ## KNN\n",
    "        knn_parameters = {'n_neighbors': range(1, 10), 'weights': ['uniform', 'distance']}\n",
    "        #round(len(y_train)*0.15-1))\n",
    "        knn_cv = GridSearchCV(KNeighborsRegressor(), knn_parameters, cv = tscv)\n",
    "        knn_cv.fit(X_train, y_train)\n",
    "        #knn_model = knn_cv.predict(X_test)\n",
    "        #knn_model = KNeighborsRegressor()\n",
    "        #knn_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'knn_2'] = knn_cv.predict(X_test)\n",
    "        y_params.at[window_end+1, 'knn_2_n'] = knn_cv.best_estimator_\n",
    "        \n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "        if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "            print(F'done with {window_end+1}; {count} out of {test_length}')\n",
    "        \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "    print(F'The last epiweek to be predicted is: {window_end}')\n",
    "    print(F'The total number of predicted epiweeks is: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccac213-7abe-4ac1-99b6-8c87d6beec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_regression(dataset, target_var, window_perc):\n",
    "    #print('Running for lag '+str(lag)+' step '+str(step))\n",
    "\n",
    "    #no naive for second order regression\n",
    "    #naive = create_naive(dataset, target_var)\n",
    "    #print(naive.info())\n",
    "    \n",
    "    #lagged_dataset = create_lagged_dataset(dataset, lag)\n",
    "    \n",
    "    X, y = create_X_and_y(dataset, target_var)\n",
    "    print(X.info())\n",
    "    print(y.info())\n",
    "    \n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "    print(F'The first epiweek to be predicted is: {window_end+1}')\n",
    "    \n",
    "    y_pred = create_output_dataset(y, window_end)\n",
    "    y_params = create_output_dataset(y, window_end)\n",
    "    \n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "    print(F'The initial training dataset length is: {train_length}')\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    print(F'The initial testing dataset length is: {test_length}')\n",
    "\n",
    "    regression_without_naive(X, y, window_start, window_end, y_pred, y_params, test_length)\n",
    "    #print('Completed for lag '+str(lag)+' step '+str(step))\n",
    "    clear_output(wait=False)\n",
    "    return y_pred, y_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90150674-abb0-40ae-bc97-879fc523fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_second_order_regression(target_var, pred_directory):\n",
    "    directory = os.path.join(target_var, pred_directory)\n",
    "    for filename in os.listdir(directory):\n",
    "        pred_file = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(pred_file):\n",
    "            print(pred_file)\n",
    "            \n",
    "            y_pred = pd.read_csv(pred_file, parse_dates = [0], dayfirst = True)\n",
    "            y_pred['epiweek'] = y_pred['epiweek'].apply(create_epiweek_fromstr)\n",
    "            y_pred = y_pred.set_index('epiweek')\n",
    "            y_pred_2, y_params_2 = second_order_regression(y_pred, target_var, 0.7)\n",
    "\n",
    "\n",
    "            pred_2_path = os.path.join(target_var, 'pred_2')\n",
    "            if not os.path.exists(pred_2_path):\n",
    "                os.makedirs(pred_2_path)\n",
    "            y_pred_2.to_csv(os.path.join(pred_2_path, filename))\n",
    "\n",
    "            params_2_path = os.path.join(target_var, 'params_2')\n",
    "            if not os.path.exists(params_2_path):\n",
    "                os.makedirs(params_2_path)\n",
    "            y_params_2.to_csv(os.path.join(params_2_path, filename))\n",
    "            \n",
    "\n",
    "            print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d42f9f0-5a3e-4ff2-babb-f2b7b350d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cardiovascular disease', 'Chronic respiratory disease', 'Factors influencing health status and contact with health services', 'Digestive disease', 'Endocrine disorders', 'Malignant neoplasms', 'Diabetes mellitus', 'Genitourinary disorders', 'Musculoskeletal disease', 'Infectious and Parasitic Diseases', 'Ill-defined diseases', 'Neurological and sense disorders', 'Oral Diseases', 'Other neoplasms', 'Respiratory Infection', 'Skin diseases']\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   1 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-2)]: Done   2 out of  16 | elapsed:  7.3min remaining: 51.2min\n",
      "[Parallel(n_jobs=-2)]: Done   3 out of  16 | elapsed:  7.3min remaining: 31.8min\n",
      "[Parallel(n_jobs=-2)]: Done   4 out of  16 | elapsed:  7.3min remaining: 22.0min\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of  16 | elapsed:  7.4min remaining: 16.2min\n",
      "[Parallel(n_jobs=-2)]: Done   6 out of  16 | elapsed:  7.4min remaining: 12.3min\n",
      "[Parallel(n_jobs=-2)]: Done   7 out of  16 | elapsed:  7.4min remaining:  9.5min\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  16 | elapsed:  8.5min remaining:  8.5min\n",
      "[Parallel(n_jobs=-2)]: Done   9 out of  16 | elapsed:  9.2min remaining:  7.1min\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  16 | elapsed:  9.2min remaining:  5.5min\n",
      "[Parallel(n_jobs=-2)]: Done  11 out of  16 | elapsed:  9.2min remaining:  4.2min\n",
      "[Parallel(n_jobs=-2)]: Done  12 out of  16 | elapsed: 13.7min remaining:  4.6min\n",
      "[Parallel(n_jobs=-2)]: Done  13 out of  16 | elapsed: 13.7min remaining:  3.2min\n",
      "[Parallel(n_jobs=-2)]: Done  14 out of  16 | elapsed: 13.7min remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  16 out of  16 | elapsed: 13.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done  16 out of  16 | elapsed: 13.8min finished\n"
     ]
    }
   ],
   "source": [
    "def full_second_order_regression(target_variables_file, pred_directory):\n",
    "    target_variables = []\n",
    "    with open(target_variables_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Remove linebreak which is the last character of the string\n",
    "            target_variable = line[:-1]\n",
    "            # Add item to the list\n",
    "            target_variables.append(target_variable)\n",
    "    print(target_variables)\n",
    "\n",
    "    Parallel(n_jobs=-2, verbose=51)(delayed(run_second_order_regression)(target_var, pred_directory) for target_var in target_variables)\n",
    "    \n",
    "full_second_order_regression('target_variables.txt', 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6de2f6-f09a-4865-a396-4f051c49d080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
